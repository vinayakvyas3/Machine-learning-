{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohanwadia/ML---LAB/blob/main/Logistic_Regression_using_sklearn__(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxWyu9cAEO68"
      },
      "source": [
        "def normalize(X):\n",
        "\n",
        "    #function to normalize feature matrix, X\n",
        "\n",
        "    mins = np.min(X, axis = 0)\n",
        "    maxs = np.max(X, axis = 0)\n",
        "    rng = maxs - mins\n",
        "    norm_X = 1 - ((maxs - X)/rng)\n",
        "    return norm_X"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Bt6bD98QDwXG",
        "outputId": "8efa4009-4307-4c77-f096-469d1042fafb"
      },
      "source": [
        "from sklearn import datasets, linear_model, metrics\n",
        "import csv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "dataset = pd.read_csv('/content/dataset1 (2).csv')\n",
        "dataset=np.array(dataset)\n",
        "X = normalize(dataset[:, :-1])\n",
        "\n",
        "\n",
        "# stacking columns wth all ones in feature matrix\n",
        "X = np.hstack((np.matrix(np.ones(X.shape[0])).T, X))\n",
        "print('\\n')\n",
        "print(X)\n",
        "\n",
        "# response vector\n",
        "y = dataset[:, -1]\n",
        "# splitting X and y into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4,\n",
        "                                                    random_state=1)\n",
        "\n",
        "# create logistic regression object\n",
        "reg = linear_model.LogisticRegression()\n",
        "\n",
        "# train the model using the training sets\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# making predictions on the testing set\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "# comparing actual response values (y_test) with predicted response values (y_pred)\n",
        "print(\"Logistic Regression model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred)*100)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[[1.         0.19242517 0.05065823]\n",
            " [1.         0.41640382 0.09866732]\n",
            " [1.         0.6119831  0.17599276]\n",
            " [1.         0.69084812 0.29600195]\n",
            " [1.         0.6119831  0.27999892]\n",
            " [1.         0.68453991 0.35733787]\n",
            " [1.         0.74448032 0.44266483]\n",
            " [1.         0.73501801 0.33867218]\n",
            " [1.         0.90536447 0.4826724 ]\n",
            " [1.         0.81071647 0.36800205]\n",
            " [1.         0.83911585 0.4640067 ]\n",
            " [1.         0.85488637 0.61599492]\n",
            " [1.         0.70661863 0.51200227]\n",
            " [1.         0.55520926 0.3813289 ]\n",
            " [1.         0.44479074 0.25066905]\n",
            " [1.         0.25867378 0.11999567]\n",
            " [1.         0.19242517 0.01332685]\n",
            " [1.         0.         0.        ]\n",
            " [1.         0.15142184 0.07732544]\n",
            " [1.         0.29653547 0.02932987]\n",
            " [1.         0.40694152 0.05333441]\n",
            " [1.         0.49211474 0.09600465]\n",
            " [1.         0.30284368 0.21866299]\n",
            " [1.         0.52996397 0.32266915]\n",
            " [1.         0.6687694  0.3920066 ]\n",
            " [1.         0.6687694  0.43466331]\n",
            " [1.         0.78548365 0.05065823]\n",
            " [1.         0.84226996 0.16800476]\n",
            " [1.         0.65299889 0.08266429]\n",
            " [1.         0.44794485 0.03199254]\n",
            " [1.         0.49526885 0.19467196]\n",
            " [1.         0.57728797 0.25867056]\n",
            " [1.         0.63721591 0.37332739]\n",
            " [1.         0.77917545 0.48533506]\n",
            " [1.         0.84857816 0.55999784]\n",
            " [1.         0.92428908 0.61333225]\n",
            " [1.         0.9495219  0.53333063]\n",
            " [1.         0.97790882 0.5733382 ]\n",
            " [1.         0.86750277 0.35733787]\n",
            " [1.         0.76025083 0.27999892]\n",
            " [1.         0.57413387 0.23466602]\n",
            " [1.         0.50157705 0.20533614]\n",
            " [1.         0.71608093 0.36000054]\n",
            " [1.         0.84542406 0.45600519]\n",
            " [1.         0.88958149 0.62399643]\n",
            " [1.         1.         0.7280026 ]\n",
            " [1.         0.86750277 0.55199632]\n",
            " [1.         0.70977273 0.54933366]\n",
            " [1.         0.64037001 0.48533506]\n",
            " [1.         0.22713276 0.46133052]\n",
            " [1.         0.12934312 0.335996  ]\n",
            " [1.         0.09778964 0.2453302 ]\n",
            " [1.         0.42586613 0.44000216]\n",
            " [1.         0.31546009 0.34933636]\n",
            " [1.         0.57097977 0.55467251]\n",
            " [1.         0.40063331 0.49599924]\n",
            " [1.         0.2870607  0.44000216]\n",
            " [1.         0.40063331 0.57066202]\n",
            " [1.         0.52365577 0.664004  ]\n",
            " [1.         0.49842295 0.59466656]\n",
            " [1.         0.63090771 0.71466223]\n",
            " [1.         0.60567489 0.65333982]\n",
            " [1.         0.72238914 0.75733247]\n",
            " [1.         0.60252079 0.74132944]\n",
            " [1.         0.71292683 0.80800422]\n",
            " [1.         0.78548365 0.85866245]\n",
            " [1.         0.78548365 0.76533398]\n",
            " [1.         0.9589842  0.87200281]\n",
            " [1.         0.85488637 0.81332955]\n",
            " [1.         0.85488637 0.89333117]\n",
            " [1.         0.52365577 0.86934015]\n",
            " [1.         0.41324972 0.72000108]\n",
            " [1.         0.32176829 0.62667261]\n",
            " [1.         0.18296286 0.52000378]\n",
            " [1.         0.11040604 0.40799611]\n",
            " [1.         0.         0.30666613]\n",
            " [1.         0.         0.62399643]\n",
            " [1.         0.13249723 0.69867272]\n",
            " [1.         0.06309451 0.51200227]\n",
            " [1.         0.18927107 0.76799665]\n",
            " [1.         0.24605737 0.664004  ]\n",
            " [1.         0.3880169  0.81066688]\n",
            " [1.         0.47003603 0.8186684 ]\n",
            " [1.         0.53311807 0.94400292]\n",
            " [1.         0.6624612  0.85333712]\n",
            " [1.         0.56782566 0.79466386]\n",
            " [1.         0.63406181 0.99199849]\n",
            " [1.         0.39747921 0.86133863]\n",
            " [1.         0.18611696 0.81600573]\n",
            " [1.         0.0536322  0.72533993]\n",
            " [1.         0.10409784 0.57866353]\n",
            " [1.         0.19242517 0.63199795]\n",
            " [1.         0.2839066  0.50400076]\n",
            " [1.         0.47003603 0.62667261]\n",
            " [1.         0.63406181 0.83733409]\n",
            " [1.         0.47003603 0.68799503]\n",
            " [1.         0.83280765 0.97333279]\n",
            " [1.         0.46686946 1.        ]\n",
            " [1.         0.46686946 0.94593572]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "np.matrix is not supported. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-3861b6be330a>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# train the model using the training sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# making predictions on the testing set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1194\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m   1197\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         )\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1107\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    735\u001b[0m     \"\"\"\n\u001b[1;32m    736\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m    738\u001b[0m             \u001b[0;34m\"np.matrix is not supported. Please convert to a numpy array with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;34m\"np.asarray. For more information see: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: np.matrix is not supported. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9QdqogrEVjh",
        "outputId": "99e40f33-22f8-41b8-9589-16c7b6d47e99"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6LLNfOeDwKy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fead419-b510-4ef5-feb1-a0ade7782e18"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "logmodel = LogisticRegression()\n",
        "logmodel.fit(x_train, y_train)\n",
        "\n",
        "predictions = logmodel.predict(x_test)\n",
        "print(classification_report(y_test, predictions))\n",
        "print(confusion_matrix(y_test, predictions))\n",
        "print(accuracy_score(y_test, predictions))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        63\n",
            "           1       0.95      0.95      0.95        59\n",
            "           2       0.98      1.00      0.99        55\n",
            "           3       0.97      0.96      0.96        68\n",
            "           4       0.97      0.98      0.98        66\n",
            "           5       0.96      0.90      0.93        52\n",
            "           6       1.00      1.00      1.00        54\n",
            "           7       0.98      0.95      0.97        62\n",
            "           8       0.89      0.98      0.93        51\n",
            "           9       0.95      0.94      0.94        64\n",
            "\n",
            "    accuracy                           0.97       594\n",
            "   macro avg       0.97      0.97      0.97       594\n",
            "weighted avg       0.97      0.97      0.97       594\n",
            "\n",
            "[[63  0  0  0  0  0  0  0  0  0]\n",
            " [ 0 56  0  0  1  0  0  0  2  0]\n",
            " [ 0  0 55  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 65  0  0  0  1  2  0]\n",
            " [ 0  1  0  0 65  0  0  0  0  0]\n",
            " [ 0  1  1  1  0 47  0  0  0  2]\n",
            " [ 0  0  0  0  0  0 54  0  0  0]\n",
            " [ 0  0  0  1  1  0  0 59  0  1]\n",
            " [ 0  0  0  0  0  1  0  0 50  0]\n",
            " [ 0  1  0  0  0  1  0  0  2 60]]\n",
            "0.9663299663299664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZQJdiKHLfdw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3319542a-a14d-43ef-da35-a85bf3d1ad9b"
      },
      "source": [
        "from sklearn import datasets, linear_model, metrics\n",
        "\n",
        "# load the digit dataset\n",
        "digits = datasets.load_digits()\n",
        "\n",
        "# defining feature matrix(X) and response vector(y)\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "# splitting X and y into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4,\n",
        "                                                    random_state=1)\n",
        "\n",
        "# create logistic regression object\n",
        "reg = linear_model.LogisticRegression()\n",
        "\n",
        "# train the model using the training sets\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# making predictions on the testing set\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "# comparing actual response values (y_test) with predicted response values (y_pred)\n",
        "print(\"Logistic Regression model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred)*100)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression model accuracy(in %): 96.52294853963839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6q4fhgs8uON",
        "outputId": "0823f8c9-d363-40c7-b7c9-0cbe5eb51f16"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "logmodel = LogisticRegression()\n",
        "logmodel.fit(x_train, y_train)\n",
        "\n",
        "predictions = logmodel.predict(x_test)\n",
        "print(classification_report(y_test, predictions))\n",
        "print(confusion_matrix(y_test, predictions))\n",
        "print(accuracy_score(y_test, predictions))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        63\n",
            "           1       0.95      0.95      0.95        59\n",
            "           2       0.98      1.00      0.99        55\n",
            "           3       0.97      0.96      0.96        68\n",
            "           4       0.97      0.98      0.98        66\n",
            "           5       0.96      0.90      0.93        52\n",
            "           6       1.00      1.00      1.00        54\n",
            "           7       0.98      0.95      0.97        62\n",
            "           8       0.89      0.98      0.93        51\n",
            "           9       0.95      0.94      0.94        64\n",
            "\n",
            "    accuracy                           0.97       594\n",
            "   macro avg       0.97      0.97      0.97       594\n",
            "weighted avg       0.97      0.97      0.97       594\n",
            "\n",
            "[[63  0  0  0  0  0  0  0  0  0]\n",
            " [ 0 56  0  0  1  0  0  0  2  0]\n",
            " [ 0  0 55  0  0  0  0  0  0  0]\n",
            " [ 0  0  0 65  0  0  0  1  2  0]\n",
            " [ 0  1  0  0 65  0  0  0  0  0]\n",
            " [ 0  1  1  1  0 47  0  0  0  2]\n",
            " [ 0  0  0  0  0  0 54  0  0  0]\n",
            " [ 0  0  0  1  1  0  0 59  0  1]\n",
            " [ 0  0  0  0  0  1  0  0 50  0]\n",
            " [ 0  1  0  0  0  1  0  0  2 60]]\n",
            "0.9663299663299664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    }
  ]
}